# -*- coding: utf-8 -*-
"""CogAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11ZD2e_7S8PL2sE2HRIM9XTXUcY91vtOg
"""

#load the dataset
from google.colab import files
upload = files.upload()

import pandas as pd

file = 'sample_sales_data.csv'
df = pd.read_csv(file)

#removing unwanted attribute
df.drop(columns=['Unnamed: 0'],inplace=True,errors='ignore')
#printing first 5 values
df.head(5)
df.tail(5)

"""DESCRIPTIVE STATISTICS :"""

#getting the shape of the data
df.shape

"""The dataset has Rows = 7829
                Columns = 9
"""

#getting the information about each attribute of the dataset
df.info()

import numpy as np
numeric_columns = df.select_dtypes(include=[np.number])  # Select only numeric columns
missing_values_sum = numeric_columns.isnull().sum()  # Calculate the sum of missing values
missing_values_sum

"""no missing value in numeric data columns

Looking at the output of the .info() method, we can intepret each column as follows:
*   transaction_id = this is a unique ID that is assigned to each transaction
*   timestamp = this is the datetime at which the transaction was made
*   product_id = this is an ID that is assigned to the product that was sold. Each product has a unique ID
*   category = this is the category that the product is contained within
*   customer_type = this is the type of customer that made the transaction
*   unit_price = the price that 1 unit of this item sells for
*   quantity = the number of units sold for this product within this transaction
*   total = the total amount payable by the customer
*   payment_type = the payment method used by the customer

It is also interesting to look at the datatypes. We can see that there are 3 different datatypes within this dataset:

*   object = this column contains categorical values
*   float64 = this column contains floating point numerical values (i.e. decimal numbers)
*   int64 = this column contains integer values (whole numbers)
"""

df.describe() #this gives the statistical analysis of the numeric attributes from the data

"""VISUALIZATION

Lets plot the distribution of numerical and categorical data and unique value from the dataset


1.  For continous we will use displot() from seaborn that is used to plot histograms like plots for continous values.
2.  For categrical we will use catplot() from seaborn that is used to plot categorical data.
"""

import seaborn as sns

def plot_continuous_distribution(data: pd.DataFrame = None, column: str = None, height: int = 8):
  _ = sns.displot(data, x=column, kde=True, height=height, aspect=height/5).set(title=f'Distribution of {column}');

def get_unique_values(data, column):
  num_unique_values = len(data[column].unique())
  value_counts = data[column].value_counts()
  print(f"Column: {column} has {num_unique_values} unique values\n")
  print(value_counts)

def plot_categorical_distribution(data: pd.DataFrame = None, column: str = None, height: int = 8, aspect: int = 2):
  _ = sns.catplot(data=data, x=column, kind='count', height=height, aspect=aspect).set(title=f'Distribution of {column}');



plot_continuous_distribution(df, 'unit_price')

"""This tell us that the distribution of unit_price is positively skewed, that is, there are more sales of products with a low unit_price compared to products with a high unit_price.

This makes sense, you would expect a grocery store to sell more products that are cheap, and just a few products that are really expensive.
"""

plot_continuous_distribution(df,'quantity')

"""The distribution of quantity is very different. We can see that only 4 unique values exist (1, 2, 3, and 4) and they are quite evenly distributed. It seems as though customers are buying in even quantities across 1 to 4 units"""

plot_continuous_distribution(df,'total')

"""The total follows a similar distribution to unit_price. This you may expect, as the total is calculated as unit_price x quantity.

However, this distribution is even more positively skewed. Once again, using intuition, this distribution makes sense. You'd expect customers at a grocery store to generally make more transactions of low value and only occasionally make a transaction of a very high value.
"""

#lets visualise the categorical data now
#check for unique values

get_unique_values(df, 'transaction_id')

"""since there are all unique values in transaction_id its not worth visualizing it."""

get_unique_values(df, 'product_id')

"""Similarly as of product_id but we can conclude from this that each of the 300 product has a unique id  and this tells us that how many times each product has been sold so from that the highest sold product is **ecac012c-1dec-41d4-9ebd-56fb7166f6d9**    114  and the lowest sold product is **ec0bb9b5-45e3-4de8-963d-e92aa91a201e**      3"""

get_unique_values(df, 'category')

"""There are 22 unique values for category, with fruit and vegetables being the 2 most frequently purchased product categories and spices and herbs being the least. Let's visualise this too"""

plot_categorical_distribution(df, 'category', height=10, aspect=3.5)

get_unique_values(df, 'customer_type')

plot_categorical_distribution(df, 'customer_type', height=5, aspect=1.5)

"""From the above one can conclude that the frequent buyers are the non-member,standard and the premium customer-type."""

get_unique_values(df, 'payment_type')

plot_categorical_distribution(df, 'payment_type', height=5, aspect=1.5)

get_unique_values(df, 'timestamp')

#covert to datetime(timestamp):
def convert_to_datetime(data: pd.DataFrame = None, column: str = None):

  dummy = data.copy()
  dummy[column] = pd.to_datetime(dummy[column], format='%Y-%m-%d %H:%M:%S')
  return dummy

df = convert_to_datetime(df, 'timestamp')

df.info()

#adding new coloum hour to dataset
df['hour'] = df['timestamp'].dt.hour

df.head()

get_unique_values(df, 'hour')

"""From this we can see that the 11th, 16th and 18th hour of the day are the top 3 hours of the day for transactions being processed. This is interesting, this would suggest that their busiest times of day may be just before lunch, and as people are on the way home from work

CORRELATIONS
"""

corr = df.corr()
corr.style.background_gradient(cmap='coolwarm')

"""From this correlation matrix, we can see that the only columns that have a high correlation are unit_price and total"""

